{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bna1eSO8dx_9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "import Preprocess\n",
    "import CRF\n",
    "from Parameters import *\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import keras.backend as K\n",
    "import gc\n",
    "import unicodedata\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s-ENQalSC_O"
   },
   "source": [
    "# LaBSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QrDu28vsQAu8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  558.6550425770001\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_url, max_seq_length):\n",
    "  labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "  # Define input.\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                         name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                      name=\"segment_ids\")\n",
    "\n",
    "  # LaBSE layer.\n",
    "  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "  # The embedding is l2 normalized.\n",
    "  pooled_output = tf.keras.layers.Lambda(\n",
    "      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "  # Define model.\n",
    "  return tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output), labse_layer\n",
    "start = timeit.default_timer()\n",
    "max_seq_length = 64\n",
    "labse_model, labse_layer = get_model(\n",
    "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Time:  121.4608063\n"
     ]
    }
   ],
   "source": [
    "# start = timeit.default_timer()\n",
    "# labse_model = keras.models.load_model(os.path.join(WORKING_DIR, 'Model/LaBSE_model'))\n",
    "# labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "# stop = timeit.default_timer()\n",
    "\n",
    "# print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(labse_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hub_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "# embed = hub.KerasLayer(hub_url)\n",
    "# embeddings = embed([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "# print(embeddings.shape, embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(labse_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labse_layer_weights = labse_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(labse_layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(labse_layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(labse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Gx3MhTJQDZj"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "\n",
    "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
    "  for input_string in input_strings:\n",
    "    # Tokenize input.\n",
    "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    sequence_length = min(len(input_ids), max_seq_length)\n",
    "\n",
    "    # Padding or truncation.\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "      input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    segment_ids_all.append([0] * max_seq_length)\n",
    "\n",
    "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n",
    "\n",
    "def encode(input_text):\n",
    "  input_ids, input_mask, segment_ids = create_input(\n",
    "    input_text, tokenizer, max_seq_length)\n",
    "  return labse_model([input_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvG_AifIn22_"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l9dn8rdgn7AF"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(AD_MODEL_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqNelhISg0cp"
   },
   "source": [
    "# Load necessary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZjNSeZ6ZQKfq"
   },
   "outputs": [],
   "source": [
    "norm_embeddings = np.load(NORM_EMBEDDING_FILE, allow_pickle=True)\n",
    "NT_norm_embeddings = np.load(NT_NORM_EMBEDDING_FILE, allow_pickle=True)\n",
    "\n",
    "with open(file=NORM_ADDS_FILE, mode='r', encoding='utf-8') as f:\n",
    "  NORM_ADDS = json.load(fp=f)\n",
    "\n",
    "with open(file=ID2id_FILE, mode='r', encoding='utf-8') as f:\n",
    "  ID2id = json.load(fp=f)\n",
    "\n",
    "with open(file=id2ID_FILE, mode='r', encoding='utf-8') as f:\n",
    "  id2ID = json.load(fp=f)\n",
    "\n",
    "with open(file=id2norm_add_FILE, mode='r', encoding='utf-8') as f:\n",
    "  id2norm_add = json.load(fp=f)\n",
    "dim = 772\n",
    "num_of_norm = 34481\n",
    "\n",
    "# for a sample in trainset, get id of norm_add coresponding to noisy_add of this sample\n",
    "def get_norm_id(sample):\n",
    "  return list(sample['std_add'].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cZx8jsQZ3PEj"
   },
   "outputs": [],
   "source": [
    "entities2index = {'street': 0, 'ward': 1, 'district': 2, 'city': 3}\n",
    "\n",
    "def create_type_add_vector(noisy_add):\n",
    "  entities = CRF.detect_entity(noisy_add)\n",
    "  type_add_vector = np.zeros((1,4))\n",
    "  for entity in entities:\n",
    "    if entity == 'name':\n",
    "      pass\n",
    "    else:\n",
    "      index = entities2index[entity]\n",
    "      type_add_vector[0, index] = 1\n",
    "  return type_add_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tFbHJjrq3i74"
   },
   "outputs": [],
   "source": [
    "def concat(v,type_add_vector):\n",
    "  return np.concatenate((v, type_add_vector), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImCMhR4ngn70"
   },
   "source": [
    "# Predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OivAYFdAQMuc"
   },
   "outputs": [],
   "source": [
    "class Predict:\n",
    "  def __init__(self, SNN_model, norm_embeddings, NT_norm_embeddings, dim, num_of_norm):\n",
    "    self.SNN_model = SNN_model\n",
    "    self.norm_embeddings = norm_embeddings\n",
    "    self.NT_norm_embeddings = NT_norm_embeddings\n",
    "    self.dim = dim\n",
    "    self.num_of_norm = num_of_norm\n",
    "\n",
    "  def predict(self, noisy_add):  \n",
    "    noisy_add = unicodedata.normalize('NFC', noisy_add)\n",
    "    type_add_vector = create_type_add_vector(noisy_add)\n",
    "    noisy_add = Preprocess.remove_punctuation(CRF.get_better_add(noisy_add)).lower()\n",
    "    noisy_add_vector = concat(np.array(encode([noisy_add])), type_add_vector).reshape(dim,)\n",
    "    noisy_add_vectors = np.full((num_of_norm, dim), noisy_add_vector)\n",
    "    if noisy_add == Preprocess.remove_tone_of_text(noisy_add):\n",
    "        x = model.predict([noisy_add_vectors, NT_norm_embeddings]).reshape(num_of_norm,)\n",
    "    else:\n",
    "        x = model.predict([noisy_add_vectors, norm_embeddings]).reshape(num_of_norm,)\n",
    "    \n",
    "    x = np.argmax(x, axis=0)\n",
    "    print(NORM_ADDS['data'][x]['std_add'])\n",
    "\n",
    "predictor = Predict(model, norm_embeddings, NT_norm_embeddings, dim, num_of_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1851,
     "status": "ok",
     "timestamp": 1611332832842,
     "user": {
      "displayName": "Nam Cao Hải",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCmrl3X4wfKjF82UzLyBPTNB6px2ty7jZ8llyL=s64",
      "userId": "02198006735637931468"
     },
     "user_tz": -420
    },
    "id": "0hYGBPbziCQ3",
    "outputId": "84873246-7c7a-4212-866f-ae73ac171b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'street': 'tô hiến thành', 'district': 'thanh hóa', 'city': 'thanh hóa'}\n"
     ]
    }
   ],
   "source": [
    "noisy_add = '4 - 6 Tô Hiến Thành, Tp. Thanh Hóa'\n",
    "# noisy_add = '28, Cửa Tả, P. Lam Sơn, Thành phố Thanh Hóa, T. Thanh Hóa'\n",
    "# noisy_add = '36 Lê Lợi, Thành Phố Sầm Sơn, Thanh Hóa'\n",
    "# noisy_add = 'Jun-98, Cao Bá Quát, P. Đông Thọ, Thành phố Thanh Hóa, T. Thanh Hóa'\n",
    "# noisy_add = '16, Trần Xuân Soạn, P. Đông Thọ, Thành phố Thanh Hóa, T. Thanh Hóa'\n",
    "# noisy_add = '191, Tống Duy Tân, P. Lam Sơn, Thành phố Thanh Hóa, T. Thanh Hóa'\n",
    "# noisy_add = '20 Lê Quý Đôn, Phường Ba Đình, Thanh Hóa'\n",
    "# noisy_add = 'lê quý đôn đống đa hà nội'\n",
    "# noisy_add = 'phường ba đinh tỉnh Thanh Hóa'\n",
    "predictor.predict(noisy_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ward': 'vĩnh hùng', 'district': 'vĩnh lộc', 'city': 'thanh hóa'}\n"
     ]
    }
   ],
   "source": [
    "noisy_add = 'vinh hung vinh loc thanh hoa'\n",
    "predictor.predict(noisy_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ward': 'vĩnh hùng', 'district': 'vĩnh lộc', 'city': 'thanh hóa'}\n"
     ]
    }
   ],
   "source": [
    "noisy_add = 'vinh hung vinh loc thanh hoa'\n",
    "predictor.predict(noisy_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MjncoXMhoHKG",
    "5s-ENQalSC_O",
    "MqNelhISg0cp",
    "ImCMhR4ngn70",
    "3Eq2DuMkzOdM",
    "2eoAcFnrzJfN",
    "y6dqAXFIYdoD",
    "pm7sysJfYjTo"
   ],
   "name": "Main_Predict.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python368jvsc74a57bd0e95d1dac28e16833512a618a44132cd66d5a08d73a135dcac828e532131ae6e1",
   "display_name": "Python 3.6.8 64-bit ('Siameser': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}